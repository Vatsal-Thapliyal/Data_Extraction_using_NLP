{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dba6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import glob\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import openpyxl\n",
    "from openpyxl import Workbook,load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92fae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('input.xlsx')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1e1871",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "final_stop_words = []\n",
    "\n",
    "# Define the directory path where the text files are located\n",
    "dir = 'C:\\\\Users\\\\vatsa\\\\Python\\\\StopWords'  # Replace with the actual directory path\n",
    "\n",
    "# Use the glob module to retrieve a list of text files in the directory\n",
    "file_paths = glob.glob(dir + '/*.txt')\n",
    "\n",
    "for f in file_paths:\n",
    "    with open(f, 'r') as file:\n",
    "        content = file.read()\n",
    "        words = content.split()\n",
    "        stop_words.extend(words)\n",
    "\n",
    "for x in stop_words:\n",
    "    final_stop_words.append(x.lower())\n",
    "\n",
    "final_stop_words = set(final_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9922ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unicode(scrapped_data):\n",
    "    no_unicode = []\n",
    "    for i in scrapped_data:\n",
    "        strencode = i.encode(\"ascii\", \"ignore\")\n",
    "        strdecode = strencode.decode()\n",
    "        no_unicode.append(strdecode)\n",
    "        \n",
    "    return no_unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in df.index:\n",
    "    \n",
    "    URL_ID = df['URL_ID'][ind]\n",
    "    URL = df['URL'][ind]\n",
    "    url = URL\n",
    "    \n",
    "    html = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(html.content,'html.parser') \n",
    "    \n",
    "   \n",
    "    title = soup.find_all('h1')\n",
    "    result_1 = soup.find('div',class_=['td-post-content', 'tagdiv-type'])\n",
    "    \n",
    "    if result_1 != None and len(title) >= 1:     \n",
    "        \n",
    "        para = result_1.find_all('p')\n",
    "        heading = str(title[0].text)\n",
    "        \n",
    "        paragraph=\"\"\n",
    "       \n",
    "        for p in para:\n",
    "            paragraph = paragraph + str(p.text)\n",
    "            \n",
    "        \n",
    "        scrapped_data = heading + paragraph\n",
    "        scrapped_data = scrapped_data.split()\n",
    "        scrapped_data = remove_unicode(scrapped_data)\n",
    "        scrapped_data = ' '.join(scrapped_data)\n",
    "        \n",
    "        \n",
    "        file_name = str(URL_ID)+'.txt'\n",
    "        with open('C:\\\\Users\\\\vatsa\\\\Python\\\\Extracted_words\\\\'+file_name,'w') as file:\n",
    "            words = scrapped_data.split()\n",
    "            for word in words:\n",
    "                file.write(word + ' ')\n",
    "                    \n",
    "                    \n",
    "        file.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ac460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fun(extracted_str):\n",
    "    text = extracted_str\n",
    "    tkn = word_tokenize(text)\n",
    "    return tkn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e88bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fun_sent(extracted_str):\n",
    "    text = extracted_str\n",
    "    tkn = sent_tokenize(text)\n",
    "    return tkn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b132e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(word_tokens):\n",
    "    filtered_data = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_data.append(w)\n",
    "            \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde64fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = set()\n",
    "with open('C:\\\\Users\\\\vatsa\\\\Python\\\\MasterDictionary\\\\positive-words.txt','r') as file:   \n",
    "    for line in file:       \n",
    "        for word in line.split():    \n",
    "            positive_words.add(word)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e35cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = set()\n",
    "with open('C:\\\\Users\\\\vatsa\\\\Python\\\\MasterDictionary\\\\negative-words.txt','r') as file:   \n",
    "    for line in file:       \n",
    "        for word in line.split():    \n",
    "            negative_words.add(word)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e9f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_symbols(filtered_data):\n",
    "    english_words = []\n",
    "    words = set(nltk.corpus.words.words())\n",
    "\n",
    "    for i in filtered_data:\n",
    "        i = \" \".join(w for w in nltk.wordpunct_tokenize(i) if w.lower() in words or not w.isalpha())\n",
    "        english_words.append(i);\n",
    "        \n",
    "    english_words_no_punctuations = []\n",
    "    for i in english_words:\n",
    "        i = i.translate(str.maketrans('', '', string.punctuation))\n",
    "        english_words_no_punctuations.append(i)\n",
    "    \n",
    "    no_unicode = []\n",
    "    for i in english_words_no_punctuations:\n",
    "        strencode = i.encode(\"ascii\", \"ignore\")\n",
    "        strdecode = strencode.decode()\n",
    "        no_unicode.append(strdecode)\n",
    "    \n",
    "    english_words_no_punctuations = no_unicode    \n",
    "    final_filtered_data = set(english_words_no_punctuations)\n",
    "    final_filtered_data = list(final_filtered_data)\n",
    "    for i in final_filtered_data:\n",
    "        if i=='':\n",
    "            final_filtered_data.remove(i)\n",
    "        \n",
    "    return final_filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b047146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_inbuilt_stopwords_punctuations(word_tokens):\n",
    "    inbuilt_stopwords = stopwords.words('english')\n",
    "    \n",
    "    data_no_stopwords = []\n",
    "    for item in word_tokens:\n",
    "        if item not in inbuilt_stopwords:\n",
    "            data_no_stopwords.append(item)\n",
    "            \n",
    "    no_pun_stop = []\n",
    "    for i in data_no_stopwords:\n",
    "        i = i.translate(str.maketrans('', '', string.punctuation))\n",
    "        no_pun_stop.append(i)\n",
    "        \n",
    "    return no_pun_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e3f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(word):\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\") or word.endswith(\"es\") or word.endswith(\"ed\"):\n",
    "        count -= 1\n",
    "        \n",
    "    if word.endswith(\"ted\") or word.endswith(\"tes\") or word.endswith(\"ses\") or word.endswith(\"ied\") or word.endswith(\"ies\"):\n",
    "        count += 1\n",
    "                \n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78801e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o = pd.DataFrame(columns=['URL_ID','URL','POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX_WORD_COUNT','WORD_COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH'])\n",
    "\n",
    "directory = 'C:\\\\Users\\\\vatsa\\\\Python\\\\Extracted_words'\n",
    "\n",
    "ff = os.listdir(directory)\n",
    "sorted_files = sorted(ff)\n",
    "for filename in sorted_files:\n",
    "    extracted_list = []\n",
    "    f = os.path.join(directory, filename)\n",
    "    \n",
    "    \n",
    "    if os.path.isfile(f) and filename.endswith('.txt'):\n",
    "        with open(f,'r') as file:\n",
    "            for line in file:       \n",
    "                extracted_list = line.split()\n",
    "        file.close()\n",
    "        \n",
    "    \n",
    "    extracted_str = ' '.join(extracted_list)\n",
    "\n",
    "    word_tokens = tokenize_fun(extracted_str)\n",
    "    sentence_tokens = tokenize_fun_sent(extracted_str)  \n",
    "    filtered_data = filter_stopwords(word_tokens)\n",
    "    final_filtered_data = filter_symbols(filtered_data)\n",
    "    \n",
    "        # 1.Positive score\n",
    "    p_score = 0\n",
    "    for item in final_filtered_data:\n",
    "        if item in positive_words:\n",
    "            p_score=p_score+1    \n",
    "        \n",
    "        # 2.Negative score\n",
    "    n_score = 0;\n",
    "    for item in final_filtered_data:\n",
    "        if item in negative_words:\n",
    "            n_score = n_score+1\n",
    "          \n",
    "        # 3.Polarity score\n",
    "    pol_score = 0\n",
    "    pol_score =(p_score-n_score)/((p_score + n_score)+0.000001)\n",
    "    \n",
    "        # 4.Subjectivity score\n",
    "    sub_score = 0\n",
    "    sub_score = (p_score+n_score)/len(extracted_list) + 0.000001\n",
    "    \n",
    "    # word length, sentence length \n",
    "    word_length = len(word_tokens)\n",
    "    sentence_length = len(sentence_tokens)\n",
    "    \n",
    "    # 5.Average sentence length 8. average no. of words per sectence\n",
    "    average_sentence_length = word_length/sentence_length\n",
    "    average_no_of_words_per_sentence = average_sentence_length\n",
    "    \n",
    "    #9. Complex Words\n",
    "    complex_words = 0\n",
    "    for word in word_tokens:\n",
    "        syllables = syllable_count(word)\n",
    "        if syllables > 2:\n",
    "            complex_words = complex_words + 1\n",
    "            \n",
    "            #total syllables in article\n",
    "            total_syllables_in_article = 0\n",
    "            total_syllables_in_article = total_syllables_in_article + syllables\n",
    "            \n",
    "    #10. Word Count (no inbuilt stopwords, no punctuations)\n",
    "    no_pun_stop = filter_inbuilt_stopwords_punctuations(word_tokens)\n",
    "    word_count = len(no_pun_stop)\n",
    "    \n",
    "    # 11.Average syllables per word\n",
    "    syllables_per_word = total_syllables_in_article/word_length\n",
    "    \n",
    "    # 6.percentage of complex words\n",
    "    complex_percent = complex_words/word_length\n",
    "    \n",
    "    # 7.fog index\n",
    "    fog_index = 0.4*(average_sentence_length + complex_percent)\n",
    "    \n",
    "    # 12.Personal pronoun using regex\n",
    "    count_pronouns = 0\n",
    "    pronouns = r\"\\b(he|she|it|we|they|you|I)\\b\" #US not included\n",
    "    matches = re.findall(pronouns, scrapped_data , re.IGNORECASE)\n",
    "    count_pronouns = len(matches)\n",
    "    \n",
    "    # 13.Average Word length\n",
    "    total_characters = 0\n",
    "    for item in word_tokens:\n",
    "        for character in item:\n",
    "            total_characters = total_characters + 1\n",
    "            \n",
    "    average_word_length = total_characters/word_length\n",
    "    \n",
    "    url_id_ = filename[:-4]\n",
    "    url = df.loc[df['URL_ID']==int(url_id_),'URL'].iloc[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    new_row = {'URL_ID' : url_id_ ,'URL' : url,'POSITIVE SCORE' : p_score,'NEGATIVE SCORE' : n_score,'POLARITY SCORE' : pol_score,'SUBJECTIVITY SCORE' : sub_score,'AVG SENTENCE LENGTH' : average_sentence_length,'PERCENTAGE OF COMPLEX WORDS' : complex_percent,'FOG INDEX' : fog_index,'AVG NUMBER OF WORDS PER SENTENCE' : average_no_of_words_per_sentence ,'COMPLEX_WORD_COUNT' : complex_words,'WORD_COUNT' : word_count,'SYLLABLE PER WORD' : syllables_per_word,'PERSONAL PRONOUNS' : count_pronouns,'AVG WORD LENGTH' : average_word_length}\n",
    "    \n",
    "    new_row_df = pd.DataFrame([new_row])\n",
    "    df_o = pd.concat([df_o,new_row_df], ignore_index = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc273b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce85f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o.to_excel('Output.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
